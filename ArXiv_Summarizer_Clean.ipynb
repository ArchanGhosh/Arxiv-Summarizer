{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO9x0DNx1u5x"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install langchain\n",
        "!pip install arxiv\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "h74mXGnJ4wpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doc_loader(search_query):\n",
        "  try :\n",
        "    docs = ArxivLoader(query=search_query, load_max_docs=1).load()\n",
        "    return docs[0].metadata, docs[0].page_content\n",
        "  except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "dfG3gIL12GOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip(content):\n",
        "  content = str(content)\n",
        "  #print(content)\n",
        "  content = content.split(\"\\n\")\n",
        "  content = \" \".join(content)\n",
        "  #print(content)\n",
        "\n",
        "  return content"
      ],
      "metadata": {
        "id": "GfRBNN662kF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip(content):\n",
        "  loc_intro = content.find(\"Introduction\")\n",
        "  loc_refer = content.rfind(\"Reference\")\n",
        "  if loc_intro !=-1:\n",
        "    if loc_refer !=-1:\n",
        "      content = content[loc_intro:loc_refer]\n",
        "    else:\n",
        "      content = content[loc_intro:]\n",
        "      print(\"Warning: Paper Doesn't have a References Title, may lead to overlap of references in summary\")\n",
        "  else:\n",
        "    print(\"Warning: Paper Doesn't Have an Introduction Title, these may lead to overlap of summarization\")\n",
        "\n",
        "  return content"
      ],
      "metadata": {
        "id": "tx8lTO5aAq1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(content):\n",
        "\n",
        "  content = clip(content)\n",
        "\n",
        "  sent = []\n",
        "  c= 0\n",
        "  k = \"\"\n",
        "  content = content.split(\". \")\n",
        "  for i in range(len(content)):\n",
        "    k = k + content[i] + \". \"\n",
        "    c = c+1\n",
        "    if c == 10:\n",
        "      sent.append(k)\n",
        "      c = 0\n",
        "      k = \"\"\n",
        "    elif i==len(content)-1:\n",
        "      sent.append(k)\n",
        "\n",
        "  return sent"
      ],
      "metadata": {
        "id": "cAm-Ghci26Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(sent):\n",
        "  model_str = \"Falconsai/text_summarization\"\n",
        "  tokenizer_str = \"Falconsai/text_summarization\"\n",
        "\n",
        "  summarizer = pipeline(\"summarization\", model=model_str, tokenizer = tokenizer_str)\n",
        "\n",
        "\n",
        "  summarized = \"\"\n",
        "  for i in sent:\n",
        "    s = summarizer(i, max_length=256, min_length=64, do_sample=False)\n",
        "    summarized = summarized + s[0]['summary_text'] +\"\\n\"\n",
        "\n",
        "  return summarized"
      ],
      "metadata": {
        "id": "4QRmVEZY3Faj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(search_query):\n",
        "  metadata, page_content = doc_loader(search_query)\n",
        "  content = strip(page_content)\n",
        "  sent = chunk(content)\n",
        "  summarized = summarize(sent)\n",
        "\n",
        "  print(metadata['Published'])\n",
        "  print(metadata['Title'])\n",
        "  print(metadata['Authors'])\n",
        "\n",
        "  print(summarized)"
      ],
      "metadata": {
        "id": "qeI0XQq-3_2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_query = \"Attention is all you need\"\n",
        "run(search_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD8YLIfg5lrK",
        "outputId": "95a86313-9bc0-44b7-eb64-35b0e2d185f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-07-16\n",
            "All the attention you need: Global-local, spatial-channel attention for image retrieval\n",
            "Chull Hwan Song, Hye Joo Han, Yannis Avrithis\n",
            "Instance-level image retrieval is at the core of visual rep- resentation learning . Many large-scale open datasets [3, 37, 16, 29, 53] and competitions1 have accelerated progress . Some studies focus on learning features from convolutional neural networks . Others focus on re-ranking, for instance by graph-based methods .\n",
            "Each study has been limited to one or two kinds of attention only . It is the objective of our work to perform a compre-hensive study of all forms of attention above, apply them to instance-level image retrieval and provide a detailed ac- count of their interaction and impact on performance . Local at-tention is about individual locations and channels; global is about interaction between locations and between channels .\n",
            "Each of the global and local attention mechanisms comprises both spatial and channel attention . Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-tion (channels/locations weighted independently, based on contextual information obtained by pooling)\n",
            "Studies on global descriptors focus on spatial pooling of CNN feature maps into vectors, including MAC [38], SPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37], and NetVLAD [1, 25], as well as learning the representa-tion [3, 15, 16, 36, 37]. Studies before deep learning dom- inated image retrieval were mostly based on local descrip- tors like SIFT [27], bag-of-words\n",
            "CRN [25] ap- plies spatial attention for feature reweighting and is learned . Learned spatial attention mechanisms are common for local descriptors . In image classification, non-local neural network (NL- Net) [52] is maybe the first global attention mechanism, fol- lowed by similar studies [4, 59, 34].\n",
            "Table 1 attempts to categorize related work on atten-tion according to whether attention is local or global, spa-tial or channel . Of those studies that focus on image retrieval, many are not learned [23, 17, 24], and of those that are, some are de- signed for local descriptors [29, 47]. By contrast, we provide a comprehensive study of all forms of attention, global and local, spatial and channel, to obtain a learned representation in the form of a tensor that can\n",
            "Local attention collects con- text from the image and applies pooling to obtain a c11 local channel attention map Al c and a 1 h  w local spa-tial attention map Ag s . The feature maps produced by the two attention streams are combined with the original one by a learned fusion mechanism into the global-local attention feature map Fgl . Local attention Following ECA-Net [51], this attention captures local channel information.\n",
            "Channel attention is then captured by a 1D convolution of kernel size k along the channel di- mension, where k controls the extent of cross-channel inter-action . This is followed by the sigmoid function, resulting in the c  1 local channel attention map Al c. Local spatial attention Inspired by the inception mod- ule [43] and similar to [25], this attention map captures local spatial information at different scales .\n",
            "Local attention map We use the local channel attention map Al s to weigh Fl c in the channel dimension Fl := F  Al c + F . (2) Here, AB denotes an element-wise multiplication of ten- sors A and B . We adopt the choice of applying channel followed by spa-tial attention from convolutional block attention module CBAM [54]\n",
            "This mechanism is based on the non-local neural net- work [52], but with the idea of 1D convolution from ECA- Net [51] . We apply GAP and squeeze spatial dimensions, followed by a sigmoid function . The value tensor Vc is obtained by mere reshaping of F to hwc, without GAP . In GSoP [14] and A2-Net [9] this attention map is multiplied with Vc \n",
            "By using 11 convolutions, we obtain câ€²  hw query Qs, key Ks, and value Vs tensors, where each col- umn is a feature vector corresponding to a particular spatial location . We capture pairwise similarities of these vectors by matrix multiplication of Ks and Qs . This attention map is multiplied with Vs and the matrix product VsAg s is reshaped back to chw by expanding\n",
            "We combine the local and global attention feature maps, Fl and Fg, with the original feature F . We use a weighted average with weights wl, wg and w respectively, ob-tained by softmax over three learnable scalar parameters . Pooling We apply GeM [37], a learnable spatial pooling mechanism, to feature map Fgl (7), followed by a fully- connected (FC) layer with dropout and batch normalization . The final\n",
            "clean sets were obtained from the original noisy sets for more effective training [16, 53]. The original noisy datasets are much larger, but they have high intra-class variability . Red (blue) means higher (lower) attention weight. Each class can include visually dissimilar images such as exterior and interior views of a building or landmark, in-cluding floor plans and paintings inside .\n",
            "We use four common eval- uation datasets for landmark image retrieval . Oxford5k (Ox5k) [32], Paris6k [33], Revisited Ox- ford (ROxford or ROxf) and Paris (RParis or RPar) [35]. We evaluate using mean Average Pre-cision (mAP) and mean precision at 10 (mP@10).\n",
            "We adopt ArcFace [10], a cosine-softmax based loss . We adopt the batch sampling of Yokoo et al. [56] where mini-batch samples with similar aspect ratios are resized to a particular size . For image augmentation, we apply scaling, random cropping, and var-ied illumination .\n",
            "Using the backbone model alone is denoted as GLAM (global-local atten- tion module) It is compatible with recent models based on ResNet101-GeM trained with ArcFace [53, 28]. Adding our local attention (subsection 3.1) to the baseline model is referred to as +local, while adding our global attention (Subsec-tion 3.2) is dedenoted +global . Since we focus on representa-tion learning, we do not\n",
            "GLDv2-noisy has 2.6 times more images than SOLAR [28] . NC-clean has the worst performance despite being clean, aparently because it is 6 METHOD TRAIN SET DIM BASE MEDIUM HARD Ox5k Par6k ROxf +R1M RPar . GLAM-GeM-R101-Siamese [36, 35] [O] 2048 â€“ â€“ 41.7 65.0 24.2 43.7\n",
            "Weyand et al. [53] is the only model other than ours trained on GLDv2-clean . [28] is trained on gLDv1-noisy and compared in Table 3. too small . To achieve best possible performance, we use com- pare methods as a training set .\n",
            "Table 3 shows that our best model trained on GLDv2-clean outperforms [53] by a large margin . But the most impor- tant comparison is with SOLAR [28] also based on self- attention . Both local and global atten-tion bring significant gain over the baseline .\n",
            "Figure 7 shows some METHOD OXF5K PAR6K RMEDIUM RHARD ROxf RPar GLAM baseline 91.9 94.5 72.8 84.2 49.9 69.7 +local-channel 91.3 95.3 72.2 85.8 48.3 73.1 +local spatial 91.0 95.1 72.1 85.3 48.3. 71.9 +local 91.2 95.4 73.7 86.5 52.6 75.0 +global-channel 9.2 94.4 73.3 84.4 49.8\n",
            "Table 5 shows the results, which are more fine-grained than those of Table 4. We observe that, when used alone, the channel and spatial variants of local and global attention are harmful in most cases . Even the combination, baseline+global, is impressive, bringing gain of up to 7.5% . This result shows the necessity of local attention in the final model .\n",
            "Table 6 shows that the CBAM style module is considerably worse than ours on all benchmarks except Paris6k, where it is only slightly better . Concatenation vs. sum for feature fusion We use a softmax-based weighted average of local and global atten-tion feature maps with the original feature map (7).\n",
            "We use the multi-resolution representa-tion [16] for the final feature of an image at inference time . This method: (1) resizes an image into multiple scales; (2) extracts features from the resized images; (3) averages the features . The method is applied to query and database images to en- hance ranking results .\n",
            "Table 9 compares the four cases of applying this method or not to query or database images . It is manifested as a network architecture consisting of global and local atten- tion components, each operating on both spatial and chan- nel dimensions . The output is a modified feature tensor that can be used in any way, for instance with local feature detection instead of spatial pooling for image retrieval .\n",
            "one may even envision an extension to language models, where transformers originate from [50] . transformers originated from [60] and originate from . [50], a language model in which transformers are used to construct language models . . One might even envision a way to extend language models to languages .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NjBETeS99Tvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}